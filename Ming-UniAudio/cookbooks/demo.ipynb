{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4770bf52-16aa-486e-9a5e-dad64a8ad658",
   "metadata": {
    "execution": {
     "shell.execute_reply.end": "2025-10-27T07:11:40.070812Z",
     "shell.execute_reply.started": "2025-10-27T07:11:40.067295Z",
     "to_execute": "2025-10-27T07:11:40.179Z"
    },
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Ming-Lite-UniAudio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /path/to/workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28108cc-d941-4910-ac00-6106e68c2d99",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00bfc4a-38b2-4afa-899b-1ea0c6483c7b",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-27 15:05:24,009] [INFO] [add_hooks.py:71:__init__] If you are working on AIStudio, please set 'ANTMONITOR_TFEVENT_PATH' env.\n",
      "2025-10-27 15:05:24,930 - datasets - INFO - PyTorch version 2.6.0 available.\n",
      "BailingMMNativeForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce4754e6cda4c57bbb5931216f34c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at inclusionAI/Ming-UniAudio-16B-A3B were not used when initializing BailingMMNativeForConditionalGeneration: ['audio.decoder.semantic_model.conv1.bias', 'audio.decoder.semantic_model.conv1.weight', 'audio.decoder.semantic_model.conv2.bias', 'audio.decoder.semantic_model.conv2.weight', 'audio.decoder.semantic_model.positional_embedding']\n",
      "- This IS expected if you are initializing BailingMMNativeForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BailingMMNativeForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "BailingMMNativeForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "BailingMMNativeForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly defined. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Fusing experts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [01:12<00:00,  2.59s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display\n",
    "import ipynbname\n",
    "notebook_path = ipynbname.path()\n",
    "current_dir = os.path.dirname(notebook_path)\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from modeling_bailingmm import BailingMMNativeForConditionalGeneration\n",
    "import random\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from sentence_manager.sentence_manager import SentenceNormalizer\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "def seed_everything(seed=1895):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class MingAudio:\n",
    "    def __init__(self, model_path, lora_path=None, device=\"cuda:0\", use_grouped_gemm=True):\n",
    "        self.device = device\n",
    "        self.model = BailingMMNativeForConditionalGeneration.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        if use_grouped_gemm and not self.model.config.llm_config.use_grouped_gemm:\n",
    "            self.model.model.fuse_experts()\n",
    "\n",
    "        if lora_path is not None:\n",
    "            self.model = PeftModel.from_pretrained(self.model, lora_path)\n",
    "        self.model = self.model.eval().to(torch.bfloat16).to(self.device)\n",
    "        self.processor = AutoProcessor.from_pretrained(\".\", trust_remote_code=True)\n",
    "        self.tokenizer = self.processor.tokenizer\n",
    "        self.sample_rate = self.processor.audio_processor.sample_rate\n",
    "        self.patch_size = self.processor.audio_processor.patch_size\n",
    "        self.normalizer = self.init_tn_normalizer(tokenizer=self.tokenizer)\n",
    "\n",
    "    def init_tn_normalizer(self, config_file_path=None, tokenizer=None):\n",
    "\n",
    "        if config_file_path is None:\n",
    "            default_config_path = \"sentence_manager/default_config.yaml\"\n",
    "            config_file_path = default_config_path\n",
    "        with open(config_file_path, 'r') as f:\n",
    "            self.sentence_manager_config = yaml.safe_load(f)\n",
    "        if \"split_token\" not in self.sentence_manager_config:\n",
    "            self.sentence_manager_config[\"split_token\"] = []\n",
    "        assert isinstance(self.sentence_manager_config[\"split_token\"], list)\n",
    "        if tokenizer is not None:\n",
    "            self.sentence_manager_config[\"split_token\"].append(re.escape(tokenizer.eos_token))\n",
    "        normalizer = SentenceNormalizer(self.sentence_manager_config.get(\"text_norm\", {}))\n",
    "        \n",
    "        return normalizer\n",
    "\n",
    "    def speech_understanding(self, messages, lang=None):\n",
    "        text = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs, audio_inputs = self.processor.process_vision_info(messages)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            audios=audio_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "        \n",
    "        if lang is not None:\n",
    "            language = torch.tensor([self.tokenizer.encode(f'{lang}\\t')]).to(inputs['input_ids'].device)\n",
    "            inputs['input_ids'] = torch.cat([inputs['input_ids'], language], dim=1)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, dtype=attention_mask.dtype)\n",
    "        for k in inputs.keys():\n",
    "            if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "                inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "        logger.info(f\"input: {self.tokenizer.decode(inputs['input_ids'].cpu().numpy().tolist()[0])}\")\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=self.processor.gen_terminator,\n",
    "        )\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "\n",
    "        return output_text\n",
    "\n",
    "    def speech_generation(\n",
    "        self, \n",
    "        text,\n",
    "        prompt_wav_path,\n",
    "        prompt_text,\n",
    "        lang='zh',\n",
    "        output_wav_path='out.wav'\n",
    "    ):\n",
    "        text = self.normalizer.normalize(text)\n",
    "        waveform = self.model.generate_tts(\n",
    "            text=text,\n",
    "            prompt_wav_path=prompt_wav_path,\n",
    "            prompt_text=prompt_text,\n",
    "            patch_size=self.patch_size,\n",
    "            tokenizer=self.tokenizer,\n",
    "            lang=lang,\n",
    "            output_wav_path=output_wav_path,\n",
    "            sample_rate=self.sample_rate,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "    def speech_edit(\n",
    "        self, \n",
    "        messages,\n",
    "        output_wav_path='out.wav',\n",
    "        use_cot=True\n",
    "    ):\n",
    "        text = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs, audio_inputs = self.processor.process_vision_info(messages)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            audios=audio_inputs,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        if use_cot:\n",
    "            ans = torch.tensor([self.tokenizer.encode('<answer>')]).to(inputs['input_ids'].device)\n",
    "            inputs['input_ids'] = torch.cat([inputs['input_ids'], ans], dim=1)\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, dtype=attention_mask.dtype)\n",
    "        for k in inputs.keys():\n",
    "            if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\n",
    "                inputs[k] = inputs[k].to(dtype=torch.bfloat16)\n",
    "        logger.info(f\"input: {self.tokenizer.decode(inputs['input_ids'].cpu().numpy().tolist()[0])}\")\n",
    "\n",
    "        edited_speech, edited_text = self.model.generate_edit(\n",
    "            **inputs,\n",
    "            tokenizer=self.tokenizer,\n",
    "            output_wav_path=output_wav_path\n",
    "        )\n",
    "        return edited_speech, edited_text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # model = MingAudio(\"/path/to/model\")\n",
    "    # load base model\n",
    "    model = MingAudio(\"inclusionAI/Ming-UniAudio-16B-A3B\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c3e1e-690a-4232-94b6-3d0ce26879fa",
   "metadata": {
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c71f9-336a-433a-9e4a-a6980b938e4d",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 20:54:26.673\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>\u001b[0m\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "2025-09-30 20:54:27,589 - modeling_bailing_moe - WARNING - The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "\u001b[32m2025-09-30 20:54:28.285\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: Chinese\tç°åœ¨æ˜¯ä¸æ˜¯ä¹Ÿè¯¥é•¿ç‚¹å¿ƒäº†å§\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/BAC009S0915W0292.wav\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages)\n",
    "    logger.info(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9ed2d-0eb5-4720-b9bc-4e5555ff8827",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": false,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-01 21:47:27.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>å·æ¸\t\u001b[0m\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "2025-10-01 21:47:28,317 - modeling_bailing_moe - WARNING - The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "\u001b[32m2025-10-01 21:47:28.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: æˆ‘éš¾å—å¾—å¾ˆåˆ«ä¸ªéƒ½ç¡äº†\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:28.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>æ¹–å—\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:29.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mGenerated Response: æˆ‘ä»¬åˆ°ä½ å±‹é‡Œå¤§æ¦‚ä¸€ç‚¹åŠå·¦å³\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:29.636\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>é—½å—\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mGenerated Response: å®è´è¾ƒæ—©ä¼‘å›°æ™šå®‰\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>ä¸Šæµ·\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mGenerated Response: é˜¿æ‹‰è€ƒè¯•è¿˜æ²¡å®šä¸‹æ¥å”»\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:30.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral.<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>Canton\t\u001b[0m\n",
      "\u001b[32m2025-10-01 21:47:31.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1mGenerated Response: ä½ åšä¹œå˜¢å•Šç³»å’ªå””æƒ³å€¾åˆå•Š\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Dialect ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/chuanyu_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"å·æ¸\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/hunan_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"æ¹–å—\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/minnan_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"é—½å—\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/shanghai_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"ä¸Šæµ·\")\n",
    "    logger.info(f\"Generated Response: {response}\")\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/yueyu_demo.wav\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages, lang=\"Canton\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc7088-a1ec-4e17-84d3-6047a30a6557",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating zh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<role>HUMAN</role>Please translate the text to speech.\n",
      "åœ¨æ­¤å¥‰åŠå¤§å®¶åˆ«ä¹±æ‰“ç¾ç™½é’ˆã€‚æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯æ„å»ºæœªæ¥æœåŠ¡ä¸šçš„æ•°å­—åŒ–åŸºç¡€è®¾æ–½ï¼Œä¸ºä¸–ç•Œå¸¦æ¥æ›´å¤šå¾®å°è€Œç¾å¥½çš„æ”¹å˜ã€‚<role>ASSISTANT</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 96/300 [00:13<00:27,  7.37it/s]\n",
      "\u001b[32m2025-09-30 20:56:01.995\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mwaveform: tensor([[ 5.7267e-04,  1.2557e-03,  8.2468e-04,  ..., -2.2724e-06,\n",
      "         -1.6844e-06, -2.2619e-05]])\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopInfo: 96 299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # TTS\n",
    "    waveform = model.speech_generation(\n",
    "        text='æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯æ„å»ºæœªæ¥æœåŠ¡ä¸šçš„æ•°å­—åŒ–åŸºç¡€è®¾æ–½ï¼Œä¸ºä¸–ç•Œå¸¦æ¥æ›´å¤šå¾®å°è€Œç¾å¥½çš„æ”¹å˜ã€‚',\n",
    "        prompt_wav_path='data/wavs/10002287-00000094.wav',\n",
    "        prompt_text='åœ¨æ­¤å¥‰åŠå¤§å®¶åˆ«ä¹±æ‰“ç¾ç™½é’ˆã€‚',\n",
    "        output_wav_path='data/output/tts.wav',\n",
    "    )\n",
    "    logger.info(f\"waveform: {waveform}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dcaf98-133e-4f77-83e6-4b9c7b1eb096",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 20:57:03.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And insert 'å®ç°' before the character or word at index 3.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 20:57:10.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0006, -0.0005, -0.0001,  ...,  0.0016,  0.0013,  0.0012]]],\n",
      "       device='cuda:0'), '<cot_text>æœ‰æœ›<edit><framePatch></edit>ç›˜æ´»è¶…ä¸‡äº¿çš„å›½ä¼å­˜é‡èµ„äº§</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # load sft model\n",
    "    model = MingAudio(\"inclusionAI/Ming-UniAudio-16B-A3B-Edit\")\n",
    "    # Ins\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And insert 'å®ç°' before the character or word at index 3.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/ins.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813319d-6664-4acd-b5b2-62dedeb905c0",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:01:13.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And delete the characters or words from index 5 to index 8.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:01:17.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0020, -0.0023, -0.0018,  ..., -0.0025, -0.0023, -0.0020]]],\n",
      "       device='cuda:0'), '<cot_text>æœ‰æœ›ç›˜æ´»<edit></edit>å›½ä¼å­˜é‡èµ„äº§</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Del\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And delete the characters or words from index 5 to index 8.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/del.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81350375-59fd-47e6-a145-bd8abcc397c8",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:02:49.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And substitute 'ç›˜æ´»' with 'åˆ›é€ '.\n",
      "</prompt><role>ASSISTANT</role><answer>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:02:55.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0007, -0.0006, -0.0002,  ...,  0.0024,  0.0020,  0.0019]]],\n",
      "       device='cuda:0'), '<cot_text>æœ‰æœ›<edit><framePatch></edit>è¶…ä¸‡äº¿çš„å›½ä¼å­˜é‡èµ„äº§</cot_text><gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Sub\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And substitute 'ç›˜æ´»' with 'åˆ›é€ '.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, output_wav_path=\"data/output/sub.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e50ebb-6600-46fe-8135-1bb58b202c81",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:05:08.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And denoise the audio.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:05:22.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-8.8062e-03,  1.3475e-02,  5.8888e-03,  ...,  2.6246e-06,\n",
      "          -2.5213e-06,  4.4239e-07]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Denoise\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/noreverb_fileid_0.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And denoise the audio.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/denoise.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9e33c-55ea-48cc-b03e-2ab83ed28225",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:07:10.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And adjusts the speed to 0.7.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:07:17.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[ 0.0004,  0.0006,  0.0008,  ..., -0.0003, -0.0005, -0.0004]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # time_stretch\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And adjusts the speed to 0.7.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/time_stretch.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af233811-ed22-40b1-981d-1260dc64988d",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:08:56.162\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And shifts the pitch by 3 steps.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:09:01.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-3.8837e-04, -2.6293e-04,  1.8646e-04,  ...,  4.6090e-05,\n",
      "           6.5749e-05,  1.7663e-05]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "    # pitch_shift\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And shifts the pitch by 3 steps.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/pitch_shift.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808b6e2-72ea-458d-b76e-6b4fdb7fd8b5",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:10:09.258\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And adjusts the volume to 0.6.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:10:14.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-9.3645e-04, -9.3372e-04, -6.7022e-04,  ...,  2.7338e-05,\n",
      "           2.5592e-05, -3.1757e-06]]], device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # vol\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And adjusts the volume to 0.6.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/vol.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffeeb20-f456-492a-baee-45ab515614e7",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:13:06.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And add rain to audio.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:13:11.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[-0.0146,  0.0316,  0.0304,  ..., -0.0022, -0.0320, -0.0388]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # add sound\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/00004768-00000024.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And add rain to audio.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/add_sound.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38c1b9-d6de-47b5-91b7-59753af52ece",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": false,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:11:39.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_understanding\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1minput: <role>HUMAN</role>Please recognize the language of this speech and transcribe it. Format: oral. This audio may contains the following words or phrases:Project Almanac,Dean Israelite,Jonny Weston,Sofia Black D' Elia,Sam Lerner,Jessie,Quinn,C G I,reverse engineer,timeline<audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:11:41.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mGenerated Response: English\tdean israelite said in an interview you wanted it to feel like a youtube video gone wrong mission accomplished the shaky cam during the meltdown stressful but genius\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Context ASR\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral. This audio may contains the following words or phrases:Project Almanac,Dean Israelite,Jonny Weston,Sofia Black D' Elia,Sam Lerner,Jessie,Quinn,C G I,reverse engineer,timeline\",\n",
    "                },\n",
    "                \n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/11302-4_1712960-1908016.wav\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = model.speech_understanding(messages=messages)\n",
    "    logger.info(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6818341-6680-4ada-8927-0c669648ec77",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:21:42.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And change the emotion to happy mood.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:21:47.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[0.0021, 0.0027, 0.0026,  ..., 0.0001, 0.0002, 0.0002]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # emotion\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/emotion_00004753-00000079.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And change the emotion to happy mood.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/emotion.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9a653-a479-4194-820d-254939bdcbb7",
   "metadata": {
    "execution": {},
    "isLargeOutputDisplay": true,
    "libroFormatter": "formatter-string",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-30 21:24:02.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mspeech_edit\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1minput: <role>HUMAN</role><audio><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch><audioPatch></audio><prompt>Please recognize the language of this speech and transcribe it. And change the accent of the speech to Chengdu.\n",
      "</prompt><role>ASSISTANT</role>\u001b[0m\n",
      "\u001b[32m2025-09-30 21:24:08.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGenerated Response: (tensor([[[ 0.0005,  0.0007,  0.0010,  ..., -0.0003, -0.0002, -0.0004]]],\n",
      "       device='cuda:0'), '<gen_audio>')\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # dialect conversion\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"HUMAN\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"audio\", \"audio\": \"data/wavs/emotion_00004753-00000079.wav\", \"target_sample_rate\": 16000},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<prompt>Please recognize the language of this speech and transcribe it. And change the accent of the speech to Chengdu.\\n</prompt>\",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = model.speech_edit(messages=messages, use_cot=False, output_wav_path=\"data/output/dialect_conversion.wav\")\n",
    "    logger.info(f\"Generated Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
