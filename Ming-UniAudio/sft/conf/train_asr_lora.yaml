train:
  model:
    pretrained_model_path: inclusionAI/Ming-UniAudio-16B-A3B
    freeze_semantic_module: True
    use_grouped_gemm: False
    sft_type: lora  # lora or full
    lora_config:
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: ['query_key_value', 'gate_proj', 'up_proj', 'down_proj', 'to_q', 'to_v']
  dataloader:
    num_workers: 8
    prefetch_factor: 4
    pin_memory: True
  optimizer:
    lr: 0.00001
    betas: [0.9, 0.95]
    eps: 0.00001
    weight_decay: 0.02
  lr_scheduler:
    name: constant_with_warmup  # cosine or constant_with_warmup
    num_warmup_steps: 100
    num_training_steps: 400000
  save_config:
    save_model_steps: 2000
    save_dir: exp/sft/asr_lora
  loss_weight:
    flow_loss: 0
    stop_loss: 0
    asr_loss: 1

dataset:
  data_jsonl_file: sft/data/asr.jsonl
  tokenizer: "."
  sr: 16000
  patch_size: 5
  hop_size: 320
  max_frames_in_batch: 100
  buffer_size: 1000